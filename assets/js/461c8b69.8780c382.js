"use strict";(self.webpackChunkgetstart=self.webpackChunkgetstart||[]).push([[321],{3905:(e,t,a)=>{a.d(t,{Zo:()=>i,kt:()=>m});var n=a(7294);function r(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function o(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function s(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?o(Object(a),!0).forEach((function(t){r(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):o(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function l(e,t){if(null==e)return{};var a,n,r=function(e,t){if(null==e)return{};var a,n,r={},o=Object.keys(e);for(n=0;n<o.length;n++)a=o[n],t.indexOf(a)>=0||(r[a]=e[a]);return r}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(n=0;n<o.length;n++)a=o[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(r[a]=e[a])}return r}var p=n.createContext({}),d=function(e){var t=n.useContext(p),a=t;return e&&(a="function"==typeof e?e(t):s(s({},t),e)),a},i=function(e){var t=d(e.components);return n.createElement(p.Provider,{value:t},e.children)},c="mdxType",u={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},h=n.forwardRef((function(e,t){var a=e.components,r=e.mdxType,o=e.originalType,p=e.parentName,i=l(e,["components","mdxType","originalType","parentName"]),c=d(a),h=r,m=c["".concat(p,".").concat(h)]||c[h]||u[h]||o;return a?n.createElement(m,s(s({ref:t},i),{},{components:a})):n.createElement(m,s({ref:t},i))}));function m(e,t){var a=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var o=a.length,s=new Array(o);s[0]=h;var l={};for(var p in t)hasOwnProperty.call(t,p)&&(l[p]=t[p]);l.originalType=e,l[c]="string"==typeof e?e:r,s[1]=l;for(var d=2;d<o;d++)s[d]=a[d];return n.createElement.apply(null,s)}return n.createElement.apply(null,a)}h.displayName="MDXCreateElement"},2953:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>p,contentTitle:()=>s,default:()=>u,frontMatter:()=>o,metadata:()=>l,toc:()=>d});var n=a(7462),r=(a(7294),a(3905));const o={title:"Shared Storage using Ceph",sidebar_label:"Shared Storage (Ceph)",sidebar_position:3,tags:["docker swarm"]},s=void 0,l={unversionedId:"shared-storage",id:"shared-storage",title:"Shared Storage using Ceph",description:"While Docker Swarm is great for keeping containers running (and restarting those that fail), it does nothing for persistent storage. This means if you actually want your containers to keep any data persistent across restarts, you need to provide shared storage to every docker node.",source:"@site/docs-docker-swarm/shared-storage.md",sourceDirName:".",slug:"/shared-storage",permalink:"/getstart/docker-swarm/shared-storage",draft:!1,tags:[{label:"docker swarm",permalink:"/getstart/docker-swarm/tags/docker-swarm"}],version:"current",lastUpdatedBy:"imoize",lastUpdatedAt:1691211684,formattedLastUpdatedAt:"Aug 5, 2023",sidebarPosition:3,frontMatter:{title:"Shared Storage using Ceph",sidebar_label:"Shared Storage (Ceph)",sidebar_position:3,tags:["docker swarm"]},sidebar:"tutorialSidebar",previous:{title:"Nodes",permalink:"/getstart/docker-swarm/nodes"},next:{title:"Swarm Mode",permalink:"/getstart/docker-swarm/swarm-mode"}},p={},d=[{value:"Pick a master node",id:"pick-a-master-node",level:3},{value:"Install cephadm on master node",id:"install-cephadm-on-master-node",level:3},{value:"Prepare other nodes",id:"prepare-other-nodes",level:3},{value:"Add OSDs",id:"add-osds",level:3},{value:"Setup CephFS",id:"setup-cephfs",level:3},{value:"Mount CephFS volume",id:"mount-cephfs-volume",level:3},{value:"Ceph Dashboard",id:"ceph-dashboard",level:3},{value:"Extra Options",id:"extra-options",level:4}],i={toc:d},c="wrapper";function u(e){let{components:t,...a}=e;return(0,r.kt)(c,(0,n.Z)({},i,a,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("p",null,"While Docker Swarm is great for keeping containers running (and restarting those that fail), it does nothing for persistent storage. This means if you actually want your containers to keep any data persistent across restarts, you need to provide shared storage to every docker node."),(0,r.kt)("h3",{id:"pick-a-master-node"},"Pick a master node"),(0,r.kt)("p",null,"One of your nodes will become the cephadm \"master\" node. Although all nodes will participate in the Ceph cluster, the master node will be the node which we bootstrap ceph on. It's also the node which will run the Ceph dashboard, and on which future upgrades will be processed. It doesn't matter which node you pick, and the cluster itself will operate in the event of a loss of the master node (although you won't see the dashboard)"),(0,r.kt)("h3",{id:"install-cephadm-on-master-node"},"Install cephadm on master node"),(0,r.kt)("p",null,"Run the following on the master node:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"sudo apt install cephadm ceph-common\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"sudo mkdir -p /etc/ceph\nsudo cephadm bootstrap --mon-ip 192.168.0.21\n")),(0,r.kt)("p",null,"Output will show your username and password of Ceph Dashboard, better write this down."),(0,r.kt)("admonition",{title:"note",type:"info"},(0,r.kt)("p",{parentName:"admonition"},"You also need install cephadm and ceph-common on the other nodes.")),(0,r.kt)("h3",{id:"prepare-other-nodes"},"Prepare other nodes"),(0,r.kt)("p",null,"It's now necessary to tranfer the following files to your other nodes, so that cephadm can add them to your cluster, and so that they'll be able to mount the cephfs when we're done:"),(0,r.kt)("table",null,(0,r.kt)("thead",{parentName:"table"},(0,r.kt)("tr",{parentName:"thead"},(0,r.kt)("th",{parentName:"tr",align:null},"Path on master"),(0,r.kt)("th",{parentName:"tr",align:null},"Path on non-master"))),(0,r.kt)("tbody",{parentName:"table"},(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"/etc/ceph/ceph.conf"),(0,r.kt)("td",{parentName:"tr",align:null},"/etc/ceph/ceph.conf")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"/etc/ceph/ceph.client.admin.keyring"),(0,r.kt)("td",{parentName:"tr",align:null},"/etc/ceph/ceph.client.admin.keyring")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"/etc/ceph/ceph.pub"),(0,r.kt)("td",{parentName:"tr",align:null},"/root/.ssh/authorized_keys (append to anything existing)")))),(0,r.kt)("p",null,"Back on the master node, run:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"sudo ceph orch host add <node-name> <node-ip>\n")),(0,r.kt)("p",null,"Once for each other node you want to join to the cluster. You can validate the results by running:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"sudo ceph orch host ls\n")),(0,r.kt)("h3",{id:"add-osds"},"Add OSDs"),(0,r.kt)("p",null,"Now the best improvement since the days of ceph-deploy and manual disks.. on the master node, run:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"sudo ceph orch apply osd --all-available-devices\n")),(0,r.kt)("p",null,"This will identify any (unpartitioned, unmounted) disks attached to each participating node, and configure these disks as OSDs."),(0,r.kt)("h3",{id:"setup-cephfs"},"Setup CephFS"),(0,r.kt)("p",null,"On the master node, create a cephfs volume in your cluster, by running:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"sudo ceph fs volume create data\n")),(0,r.kt)("p",null,"Ceph will handle the necessary orchestration itself, creating the necessary pool, mds daemon, etc."),(0,r.kt)("p",null,"You can watch the progress by running:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"sudo ceph fs ls\n")),(0,r.kt)("p",null,"To see the fs is configured run:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"sudo ceph -s \n")),(0,r.kt)("p",null,"Wait for HEALTH_OK"),(0,r.kt)("h3",{id:"mount-cephfs-volume"},"Mount CephFS volume"),(0,r.kt)("p",null,"On every node, create a mountpoint for the data"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"sudo mkdir /var/data\n")),(0,r.kt)("p",null,"Mount cephFS using systemd mount, since ceph mgr deployed in container and mount using fstab from cold boot sometime will not work because container not ready yet."),(0,r.kt)("p",null,"Create var-data.mount"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"sudo micro /etc/systemd/system/var-data.mount\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-conf"},"[Unit]\nDescription=Mount CephFS\nRequires=network-online.target local-fs.target docker.service\nAfter=docker.service ceph.target\nOnFailure=ceph-mount-helper.service\n\n[Mount]\nWhat=192.168.0.21:6789,192.168.0.22:6789,192.168.0.23:6789:/\nWhere=/var/data\nType=ceph\nOptions=name=admin,noatime,_netdev\nTimeoutSec= 3min\n\n[Install]\nWantedBy=multi-user.target ceph.target\n")),(0,r.kt)("p",null,"Specified OnFailure because container it's not ready yet, service helper will retry mounting cephFS until mounted."),(0,r.kt)("p",null,"Create ceph-mount-helper.service"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"sudo micro /etc/systemd/system/ceph-mount-helper.service\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-conf"},"[Unit]\nDescription=ceph mount helper for cephFS cluster mount\n\n[Service]\nType=oneshot\nExecStartPre=/usr/bin/sleep 5\nExecStart=/usr/bin/systemctl restart var-data.mount\nRestart=on-failure\nRestartSec=5s\nTimeoutStartSec=200\nTimeoutStopSec=120\nStartLimitInterval=5min\nStartLimitBurst=20\n")),(0,r.kt)("p",null,"Reload and enable service"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"sudo systemctl daemon-reload\nsudo systemctl enable var-data.mount\nsudo systemctl start var-data.mount\n")),(0,r.kt)("h3",{id:"ceph-dashboard"},"Ceph Dashboard"),(0,r.kt)("p",null,"The dashboard will be accessible at https://ip_of_master_node:8443"),(0,r.kt)("h4",{id:"extra-options"},"Extra Options"),(0,r.kt)("p",null,"Show Ceph Dashboard Config"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"sudo ceph config dump\n")),(0,r.kt)("p",null,"Change Dashboard IP"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"sudo ceph config set mgr mgr/dashboard/server_addr NEW_IP\n")),(0,r.kt)("p",null,"Change Grafana IP"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"sudo ceph dashboard set-grafana-api-url https://NEW_IP:3000\n")),(0,r.kt)("p",null,"Change Prometheus API Host"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"sudo ceph dashboard set-prometheus-api-host http://NEW_IP:9095\n")),(0,r.kt)("p",null,"Change Alertmanager API Host"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"sudo ceph dashboard set-alertmanager-api-host http://NEW_IP:9093\n")))}u.isMDXComponent=!0}}]);